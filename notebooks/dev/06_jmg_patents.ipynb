{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Patent analysis\n",
    "\n",
    "Here we explore and integrate the patent data and create a clean dataset for analysis.\n",
    "\n",
    "This includes:\n",
    "\n",
    "* Identifying Scottish organisations\n",
    "* Deciding what to do about organisation names\n",
    "* Deciding what to do about sectors\n",
    "* Merging with the strategic priority analysis\n",
    "* Documenting and saving\n",
    "\n",
    "Patent data is complex. [Data dictionary](https://github.com/nestauk/patent_analysis/raw/master/references/patstat_data_dict.pdf) and [guide](https://github.com/nestauk/patent_analysis/raw/master/references/The_Patents_Guide_2nd_edition.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../notebook_preamble.ipy\n",
    "\n",
    "from data_getters.labs.core import download_file\n",
    "import random\n",
    "from ast import literal_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dirs(name,dirs = ['raw','processed']):\n",
    "    '''\n",
    "    Utility that creates directories to save the data\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    for d in dirs:\n",
    "        if name not in os.listdir(f'../../data/{d}'):\n",
    "            os.mkdir(f'../../data/{d}/{name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is to download the data from nesta data getters\n",
    "\n",
    "def patent_download(file_path=None, progress=True):\n",
    "    \"\"\" Fetch Gateway To Research predicted industries\n",
    "\n",
    "    Repo: http://github.com/nestauk/patent_analysis\n",
    "    Commit: cb11b3f\n",
    "    File: https://github.com/nestauk/patent_analysis/blob/master/notebooks/02-jmg-patent_merge.ipynb\n",
    "\n",
    "    Args:\n",
    "        file_path (`str`, optional): Path to download to. If None, stream file.\n",
    "        progress (`bool`, optional): If `True` and `file_path` is not `None`,\n",
    "            display download progress.\n",
    "    \"\"\"\n",
    "    itemname = \"Scotland_temp/15_10_2019_patents_combined.csv\"\n",
    "    return download_file(itemname, file_path, progress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data_dict(table,name,path,sample=5):\n",
    "    '''\n",
    "    A function to output the form for a data dictionary\n",
    "    \n",
    "    Args:\n",
    "        -table (df) is the df we want to create the data dictionary for\n",
    "        -name (str) of the df\n",
    "        -path (str) is the place where we want to save the file\n",
    "        \n",
    "\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    types = [estimate_type(table[x],sample=sample) for x in table.columns]\n",
    "        \n",
    "    data_dict = pd.DataFrame()\n",
    "    data_dict['variable'] = table.columns\n",
    "        \n",
    "    data_dict['type'] = types\n",
    "    \n",
    "    data_dict['description'] = ['' for x in data_dict['variable']]\n",
    "        \n",
    "    out = os.path.join(path,f'{today_str}_{name}.csv')\n",
    "    \n",
    "    #print(data_dict.columns)\n",
    "    \n",
    "    data_dict.to_csv(out)\n",
    "    \n",
    "\n",
    "def estimate_type(variable,sample):\n",
    "    '''\n",
    "    Estimates the type of a column. \n",
    "\n",
    "    Args:\n",
    "        variable (iterable) with values\n",
    "        sample (n) is the number of values to test\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    selection = random.sample(list(variable),sample)\n",
    "    \n",
    "    types = pd.Series([type(x) for x in selection]).value_counts().sort_values(ascending=False)\n",
    "    \n",
    "    return(types.index[0])\n",
    "\n",
    "                \n",
    "def flat_freq(a_list):\n",
    "    '''\n",
    "    Return value counts for categories in a nested list\n",
    "    \n",
    "    '''\n",
    "    return(pd.Series([x for el in a_list for x in el]).value_counts())\n",
    "\n",
    "        \n",
    "\n",
    "def flatten_list(a_list):\n",
    "    \n",
    "    return([x for el in a_list for x in el])\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_dirs('patents',['raw','processed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Read data\n",
    "\n",
    "We read a patent dataset based on the processing and analysis that we undertook [here](https://github.com/nestauk/patent_analysis)\n",
    "\n",
    "In the patent file that we read every row is a patent application and the columns contain information about it. In some cases, the columns contain lists of applicants, IPC codes and other things.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_d = patent_download()\n",
    "\n",
    "p = pd.read_csv(p_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We need to parse some of the lists in the data\n",
    "\n",
    "#These are the list variables that we need to parse\n",
    "list_vars = ['appl_psn_name','appl_person_address','appl_laua','appl_lad_name','appl_uk_postcode_long',\n",
    "           'inv_psn_name','inv_person_address','inv_laua','inv_lad_name','inv_uk_postcode_long','tf_weight','tf_techn_field_nr', 'tf_techn_field', 'ipc_class_symbol_proc_10',\n",
    "            'appl_nuts_name','inv_nuts_name','appl_nuts','inv_nuts']\n",
    "\n",
    "\n",
    "for v in list_vars:\n",
    "    \n",
    "    print(v)\n",
    "    \n",
    "    \n",
    "    p[v] = [literal_eval(x) if pd.isnull(x)==False else np.nan for x in p[v]]\n",
    "    \n",
    "    #Bring back the misssing variables\n",
    "    if any(l in v for l in ['lad','laua','nuts','ttwa']):\n",
    "        p[v] = [np.nan if type(var)!=list else np.nan if all(x=='missing' for x in var) else var for var in p[v]]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata: NUTS lookup\n",
    "\n",
    "Read [NUTS lookup](http://geoportal1-ons.opendata.arcgis.com/datasets/9b4c94e915c844adb11e15a4b1e1294d_0.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nuts = pd.read_csv('http://geoportal1-ons.opendata.arcgis.com/datasets/9b4c94e915c844adb11e15a4b1e1294d_0.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is a NUTS 2 lookup\n",
    "nuts_2_code_name_lookup = nuts.drop_duplicates('NUTS218CD').set_index('NUTS218CD')['NUTS218NM'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#And we also identify LADS in Scotland\n",
    "lads_scotland = nuts.loc[[v[0]=='S' for v in nuts['LAD18CD']]][['LAD18CD','LAD18NM']].drop_duplicates('LAD18CD').set_index('LAD18CD')['LAD18NM'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is a NUTS code - lookup name for all NUTS codes regardless of their level\n",
    "with open('../../data/aux/patstat_nuts_lookup.json','r') as infile:\n",
    "    nuts_patstat_lookup = json.load(infile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Process data\n",
    "\n",
    "Create count of applications and inventions per NUTS area in the last two years. We focus on patent families to avoid double counting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_patenting_in_nuts(df,variable,nuts_lookup,pat_fam_id='docdb_family_id'):\n",
    "    '''\n",
    "    This function creates counts of inventors and applicants in NUTS areas.\n",
    "    \n",
    "    Note that the NUTS areas are not available at an standardised level of resolution. We will prune the length of NUTS3 (length of code>4) and match with the \n",
    "    nuts2 code lookup. This also means we will throw away any patents that don't have better level of resolution than NUTS2.\n",
    "    \n",
    "    Args:\n",
    "        df (dataframe) is the df with the patent information. Each row is a patent id and the columns contain metadata, authorship etc.\n",
    "        variable (str) is the variable we want to use in the analysis\n",
    "        pat_fam_id (str) is the patent family variable that we want to focus on\n",
    "        nuts_lookup (dict) is the nuts2 code to name lookup\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    #Group by patents\n",
    "    \n",
    "    #This gives us a set of nuts regions involved in a single invention. \n",
    "    #Note that this is binary (whether a nuts region participates in an invention, rather than the number of participants)\n",
    "    #That would require a different approach using a person - patent lookup\n",
    "    \n",
    "    \n",
    "    #All this drama is because we are concatenating lists, so we need to flatten them first\n",
    "    fam = df.dropna(axis=0,subset=[variable]).groupby(pat_fam_id)[variable].apply(lambda x: list(set(flatten_list(list(x)))))\n",
    "    \n",
    "    #This gives us the distribution of activity over NUTS\n",
    "    nuts_freqs = flat_freq(fam).reset_index(drop=False)\n",
    "    \n",
    "    #Now we want to focus on NUTS2\n",
    "    nuts_freqs.columns = ['nuts','frequency']\n",
    "    \n",
    "    #Prune nuts to focus on nuts-2\n",
    "    nuts_freqs['nuts_code'] = [x[:4] if len(x)>=4 else np.nan for x in nuts_freqs['nuts']]\n",
    "    \n",
    "    #Group and aggregate over nuts 2\n",
    "    nuts_group = nuts_freqs.groupby('nuts_code')['frequency'].sum().reset_index(drop=False)\n",
    "    \n",
    "    \n",
    "    #Get the names with the lookup\n",
    "    nuts_group['nuts_name'] = [nuts_lookup[x] for x in nuts_group['nuts_code']]\n",
    "    \n",
    "    #Set the index\n",
    "    nuts_group.set_index(['nuts_name','nuts_code'],inplace=True)\n",
    "    \n",
    "    #Name the variable\n",
    "    nuts_group.columns = [variable]\n",
    "    \n",
    "    return(nuts_group)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat_nuts = pd.concat([count_patenting_in_nuts(p.loc[p['appln_filing_year']>2015],var,nuts_patstat_lookup) for var in ['inv_nuts','appl_nuts']],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat_nuts.sort_values('inv_nuts',ascending=False).head(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting -many more applications than inventors in London"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat_nuts.to_csv(f'../../data/processed/patents/{today_str}_patent_nuts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
