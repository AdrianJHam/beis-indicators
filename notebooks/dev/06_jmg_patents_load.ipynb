{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading patent data\n",
    "\n",
    "This notebook contains code to download and process patent data to generate indicators to be used in the BEIS innovation indicators project.\n",
    "\n",
    "**Note that this notebook requires access to Nesta's data production system**\n",
    "\n",
    "The PATSTAT data we are using in this analysis is proprietary and therefore it is not possible for us to make it available in a raw format\n",
    "\n",
    "Patent data is complex. [Data dictionary](https://github.com/nestauk/patent_analysis/raw/master/references/patstat_data_dict.pdf) and [guide](https://github.com/nestauk/patent_analysis/raw/master/references/The_Patents_Guide_2nd_edition.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../notebook_preamble.ipy\n",
    "\n",
    "from data_getters.labs.core import download_file\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patent_download(file_path=None, progress=True):\n",
    "    \"\"\" Fetch Gateway To Research predicted industries\n",
    "\n",
    "    Repo: http://github.com/nestauk/patent_analysis\n",
    "    Commit: cb11b3f\n",
    "    File: https://github.com/nestauk/patent_analysis/blob/master/notebooks/02-jmg-patent_merge.ipynb\n",
    "\n",
    "    Args:\n",
    "        file_path (`str`, optional): Path to download to. If None, stream file.\n",
    "        progress (`bool`, optional): If `True` and `file_path` is not `None`,\n",
    "            display download progress.\n",
    "    \"\"\"\n",
    "    itemname = \"Scotland_temp/15_10_2019_patents_combined.csv\"\n",
    "    return download_file(itemname, file_path, progress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data_dict(table,name,path,sample=5):\n",
    "    '''\n",
    "    A function to output the form for a data dictionary\n",
    "    \n",
    "    Args:\n",
    "        -table (df) is the df we want to create the data dictionary for\n",
    "        -name (str) of the df\n",
    "        -path (str) is the place where we want to save the file\n",
    "        \n",
    "\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    types = [estimate_type(table[x],sample=sample) for x in table.columns]\n",
    "        \n",
    "    data_dict = pd.DataFrame()\n",
    "    data_dict['variable'] = table.columns\n",
    "        \n",
    "    data_dict['type'] = types\n",
    "    \n",
    "    data_dict['description'] = ['' for x in data_dict['variable']]\n",
    "        \n",
    "    out = os.path.join(path,f'{today_str}_{name}.csv')\n",
    "    \n",
    "    #print(data_dict.columns)\n",
    "    \n",
    "    data_dict.to_csv(out)\n",
    "    \n",
    "\n",
    "def estimate_type(variable,sample):\n",
    "    '''\n",
    "    Estimates the type of a column. \n",
    "\n",
    "    Args:\n",
    "        variable (iterable) with values\n",
    "        sample (n) is the number of values to test\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    selection = random.sample(list(variable),sample)\n",
    "    \n",
    "    types = pd.Series([type(x) for x in selection]).value_counts().sort_values(ascending=False)\n",
    "    \n",
    "    return(types.index[0])\n",
    "\n",
    "                           \n",
    "                           \n",
    "    \n",
    "from ast import literal_eval\n",
    "\n",
    "def flat_freq(a_list):\n",
    "    '''\n",
    "    Return value counts for categories in a nested list\n",
    "    \n",
    "    '''\n",
    "    return(pd.Series([x for el in a_list for x in el]).value_counts())\n",
    "\n",
    "def emergent_load_check(path,check=True):\n",
    "    '''\n",
    "    This function checks the results from the analysis of emerging technologies in the patent / research / glass data.\n",
    "    \n",
    "    It cleans variable and category names, outputs counts by category and top keywors by sector.\n",
    "    \n",
    "    \n",
    "    Args:\n",
    "        path (str) is the path for the data. This could be modified to include a data_getter path\n",
    "        check (boolean) is whether we want to check the data or just load it\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    #Some parsing of variables\n",
    "    data = pd.read_csv(path,\n",
    "                      dtype={'appln_id':str},\n",
    "                            converters={\n",
    "                                'proc_text':literal_eval,\n",
    "                                'keywords':literal_eval,\n",
    "                                'Strategic Priority':literal_eval})\n",
    "    \n",
    "    \n",
    "    #Tidy variable names\n",
    "    data.columns = [re.sub(' ','_',x.lower()) for x in data.columns]\n",
    "    \n",
    "    #Strip whitespace\n",
    "\n",
    "    for v in ['keywords','strategic_priority']:\n",
    "        data[v] = [[x.strip().lower() for x in el] for el in data[v]]\n",
    "        \n",
    "\n",
    "    #If we want to check the data\n",
    "    if check==True:\n",
    "        \n",
    "        print('Checking')\n",
    "        print('====')\n",
    "        \n",
    "        #What are the strategic areas?\n",
    "        area_counts = flat_freq(data['strategic_priority'])\n",
    "        \n",
    "        print('Strategic area distribution')\n",
    "        print('\\n')\n",
    "        \n",
    "        print(area_counts)\n",
    "        print('\\n')\n",
    "        \n",
    "        areas = area_counts.index\n",
    "        \n",
    "        #Keyword frequencies\n",
    "\n",
    "        #Extracts the keyword distribution per area: do we have the right distributions?\n",
    "        kw_freq = [\n",
    "            flat_freq(data.loc[[area in priorities for priorities in data['strategic_priority']]]['keywords']) for area in areas]\n",
    "        \n",
    "        print('Top keyword distribution per area')\n",
    "        print('\\n')\n",
    "        \n",
    "        #Print them\n",
    "        for t,r in zip(areas,kw_freq):\n",
    "\n",
    "            print(t)\n",
    "            print('=====')\n",
    "\n",
    "            print(r.head(n=20))\n",
    "\n",
    "            print('\\n')\n",
    "    \n",
    "        print('Area combinations')\n",
    "        print('=======')\n",
    "        \n",
    "        print(pd.Series(data['strategic_priority']).value_counts())\n",
    "        \n",
    "        \n",
    "    #Return\n",
    "    return(data)\n",
    "        \n",
    "\n",
    "def flatten_list(a_list):\n",
    "    \n",
    "    return([x for el in a_list for x in el])\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Read data\n",
    "\n",
    "We read a patent dataset based on the processing and analysis that we undertook [here](https://github.com/nestauk/patent_analysis)\n",
    "\n",
    "In the patent file that we read every row is a patent application and the columns contain information about it. In some cases, the columns contain lists of applicants, IPC codes and other things.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_d = patent_download()\n",
    "\n",
    "p = pd.read_csv(p_d,dtype={'appln_id':str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p = pd.read_csv('https://nesta-data-getters.s3.eu-west-2.amazonaws.com/Scotland_temp/11_10_2019_patents_combined.csv',dtype={'appln_id':str})\n",
    "\n",
    "# #p = pd.read_csv('/Users/jmateosgarcia/Desktop/patents/patents//data/processed/11_10_2019_patents_combined.csv',dtype={'appln_id':str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We need to parse some of the lists in the data\n",
    "\n",
    "#These are the list variables that we need to parse\n",
    "list_vars = ['appl_psn_name','appl_person_address','appl_laua','appl_lad_name','appl_uk_postcode_long',\n",
    "           'inv_psn_name','inv_person_address','inv_laua','inv_lad_name','inv_uk_postcode_long','tf_weight','tf_techn_field_nr', 'tf_techn_field', 'ipc_class_symbol_proc_10',\n",
    "            'appl_nuts_name','inv_nuts_name','appl_nuts','inv_nuts']\n",
    "\n",
    "\n",
    "for v in list_vars:\n",
    "    \n",
    "    print(v)\n",
    "    \n",
    "    \n",
    "    p[v] = [literal_eval(x) if pd.isnull(x)==False else np.nan for x in p[v]]\n",
    "    \n",
    "    #Bring back the misssing variables\n",
    "    if any(l in v for l in ['lad','laua','nuts','ttwa']):\n",
    "        p[v] = [np.nan if type(var)!=list else np.nan if all(x=='missing' for x in var) else var for var in p[v]]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata: NUTS lookup\n",
    "\n",
    "Read [NUTS lookup](http://geoportal1-ons.opendata.arcgis.com/datasets/9b4c94e915c844adb11e15a4b1e1294d_0.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nuts = pd.read_csv('http://geoportal1-ons.opendata.arcgis.com/datasets/9b4c94e915c844adb11e15a4b1e1294d_0.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is a NUTS 2 lookup\n",
    "nuts_2_code_name_lookup = nuts.drop_duplicates('NUTS218CD').set_index('NUTS218CD')['NUTS218NM'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#And we also identify LADS in Scotland\n",
    "lads_scotland = nuts.loc[[v[0]=='S' for v in nuts['LAD18CD']]][['LAD18CD','LAD18NM']].drop_duplicates('LAD18CD').set_index('LAD18CD')['LAD18NM'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is a NUTS code - lookup name for all NUTS codes regardless of their level\n",
    "with open('../../../data/aux/patstat_nuts_lookup.json','r') as infile:\n",
    "    nuts_patstat_lookup = json.load(infile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate counts of activity for Scotland\n",
    "\n",
    "Strategy:\n",
    "\n",
    "* LAD labels: Label patent ids with a is in Scotland flag and extract Scottish LADs that appear in the locations\n",
    "* NUTS label: same thing\n",
    "* Create counts of activity by patent family (to focus on invention and avoid double counting)\n",
    "* Create counts of activity by LAD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Label patent ids with Scotland info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_code_vars = ['inv_laua','appl_laua','inv_nuts','appl_nuts']\n",
    "\n",
    "#This loops goes over lists of geo codes in the patents looking for Scottish codes and then also extracts them\n",
    "    \n",
    "for c in geo_code_vars:\n",
    "    \n",
    "    if 'laua' in c:\n",
    "        \n",
    "        #Is there any overlap between LADS in a patent and the Scottis list of LADS?\n",
    "        p[c+'_scotland'] = [len(set(lads)&set(lads_scotland.keys()))>0 if type(lads)==list else np.nan for lads in p[c]]\n",
    "        \n",
    "        #What are the names of the scottish LADs?\n",
    "        p[c+'_scot_names'] = [[lads_scotland[x] for x in lads if x in lads_scotland.keys()] if type(lads)==list else np.nan for lads in p[c]]\n",
    "        \n",
    "    if 'nuts' in c:\n",
    "\n",
    "        #Here we make the variable missing if the information is only available at the UK level. \n",
    "        #Otherwise Scottish shares appears underrepresented\n",
    "        \n",
    "        p[c+'_scotland'] = [np.nan if type(nuts)!=list else np.nan if (len(set(nuts))==1)&(nuts[0]=='UK') else any('UKM' in nut for nut in nuts) if type(nuts)==list else np.nan for nuts in p[c]]\n",
    "        \n",
    "        #UKM are the Scottish NUTS\n",
    "        p[c+'_scot_names'] = [[nuts_patstat_lookup[x] for x in nuts if 'UKM' in x] if type(nuts)==list else np.nan for nuts in p[c]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a couple of lists with the Scottish variables to simplify querying\n",
    "scot_vars = [x+'_scotland' for x in geo_code_vars]\n",
    "\n",
    "scot_names = [x+'_scot_names' for x in geo_code_vars]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the share of Scottish organisations in the total\n",
    "p[scot_vars].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Look at levels of activity by Scottish LAD\n",
    "p_lad_freqs = pd.concat([flat_freq(p[x].dropna()) for x in scot_names[:2]],axis=1)\n",
    "p_lad_freqs.columns = scot_names[:2]\n",
    "\n",
    "p_lad_freqs.sort_values(scot_names[0],ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create additional variables\n",
    "\n",
    "Here we create additional variables to simplify the analysis of the Scottish data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dummies for whether a patent application involves an inventor or an applicant in a Scottish LAD or NUT region\n",
    "p['scot_inv'] = [(x['inv_laua_scotland']==True) or (x['inv_nuts_scotland']==True) for pid,x in p.iterrows()]\n",
    "\n",
    "p['scot_applicant'] = [(x['appl_laua_scotland']==True) or (x['appl_nuts_scotland']==True) for pid,x in p.iterrows()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One df with dummies for whether a patent application involves an inventor in a Scottish LAD or not\n",
    "scot_lad_dummies = pd.concat(\n",
    "    [pd.Series([sc_lad in x if type(x)==list else np.nan for x in p['inv_lad_name']],name=sc_lad) for sc_lad in sorted(lads_scotland.values())],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_final = pd.concat([p,scot_lad_dummies],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lads_scotland.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_final_keep = ['appln_id','ipr_type','granted','appln_auth','appln_filing_year',\n",
    "                'docdb_family_id','nb_citing_docdb_fam',\n",
    "                'appl_psn_name', 'appl_psn_id', 'appl_psn_sector',\n",
    "                'appl_person_address', 'appl_uk_postcode_long','appl_laua', 'appl_lad_name', 'appl_nuts',\n",
    "                'inv_psn_name', 'inv_psn_id', 'inv_psn_sector', \n",
    "                'inv_person_address','inv_uk_postcode_long','inv_laua','inv_lad_name', 'inv_nuts',\n",
    "                 'appln_abstract_lg', 'appln_abstract','tf_weight', 'tf_techn_field_nr', 'tf_techn_field',\n",
    "                'ipc_class_symbol_proc_10', 'inv_nuts_name', 'appl_nuts_name',\n",
    "               'raw_ids', 'is_ai_ipo', 'inv_laua_scotland', 'inv_laua_scot_names',\n",
    "               'appl_laua_scotland', 'appl_laua_scot_names', 'inv_nuts_scotland',\n",
    "               'inv_nuts_scot_names', 'appl_nuts_scotland', 'appl_nuts_scot_names',\n",
    "               'priority_transport_aerospace', 'priority_industrial_technologies',\n",
    "               'priority_scientific_biomedical', 'priority_digital_applications',\n",
    "               'priority_data_analytics_ai', 'priority_ict',\n",
    "               'priority_environmental_technologies', 'scot_inv', 'scot_applicant',\n",
    "              'Aberdeen City', 'Aberdeenshire', 'Angus', 'Argyll and Bute',\n",
    "               'City of Edinburgh', 'Clackmannanshire', 'Dumfries and Galloway',\n",
    "               'Dundee City', 'East Ayrshire', 'East Dunbartonshire', 'East Lothian',\n",
    "               'East Renfrewshire', 'Falkirk', 'Fife', 'Glasgow City', 'Highland',\n",
    "               'Inverclyde', 'Midlothian', 'Moray', 'Na h-Eileanan Siar',\n",
    "               'North Ayrshire', 'North Lanarkshire', 'Orkney Islands',\n",
    "               'Perth and Kinross', 'Renfrewshire', 'Scottish Borders',\n",
    "               'Shetland Islands', 'South Ayrshire', 'South Lanarkshire', 'Stirling',\n",
    "               'West Dunbartonshire', 'West Lothian']\n",
    "\n",
    "\n",
    "p_final_2 = p_final[p_final_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_final_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_final_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_final_2.to_csv(f'../../data/processed/{today_str}_patent_applications_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatenate patents in Scotland\n",
    "\n",
    "scottish_em_patents = pd.concat([pd.crosstab(p_em['inv_laua_scotland'],p_em[x],normalize=1)[True] for x in priority_areas],axis=1)\n",
    "scottish_em_patents.columns = priority_areas\n",
    "\n",
    "100*scottish_em_patents.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Focus on patent families\n",
    "\n",
    "Strategy: group by patent id and sum the number of occurrences of each priority area in the patent.\n",
    "\n",
    "We can merge these with the \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to get patent families\n",
    "\n",
    "agg = p_em.groupby(['docdb_family_id'])[priority_areas].sum()\n",
    "\n",
    "#agg = pd.concat([p_em.groupby(['docdb_family_id'])[p].sum() for p in priority_areas],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat_fam_id = agg.apply(lambda x: x.astype(int),axis=0)\n",
    "\n",
    "pat_fam_id.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_em['has_scotland'] = p_em['']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get individual company /organisation information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading it from the patents file...Should replace this \n",
    "\n",
    "pat_person = pd.read_csv('/Users/jmateosgarcia/Desktop/patents/patents/data/processed/11_10_2019_person_profiles.csv',\n",
    "                        dtype={'appln_id':str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat_person['scot_lad'] = [x[0]=='S' if type(x)==str else np.nan for x in pat_person['laua']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat_person_em = pd.merge(pat_person,em,left_on='appln_id',right_on='appln_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat_person_grouped = pat_person_em.groupby(['person_id','person_name'])[priority_areas].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat_person_grouped.sort_values('priority_data_analytics_ai',ascending=False).head(n=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above can be subset by sector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some validation\n",
    "\n",
    "How do these figures compare with European patenting stats?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat = pd.read_csv('https://ec.europa.eu/eurostat/estat-navtree-portlet-prod/BulkDownloadListing?file=data/pat_ep_rtot.tsv.gz',\n",
    "                  compression='gzip',delimiter='\\t',na_values=[': '])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat_n = pat.loc[['NR,' in x for x in pat.iloc[:,0]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat_n['geo'] = [x.split(',')[1] for x in pat_n.iloc[:,0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scot = pat_n.loc[['UKM' in x for x in pat_n['geo']]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scot.columns = [x.strip() for x in scot.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scot['2011'].astype(float).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p['appl_laua_scotland'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
