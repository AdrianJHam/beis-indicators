{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ASHE places\n",
    "\n",
    "We collect data about median salaries in a NUTS2 area. This is an indicator in its own right, and we will also use it to calculate the House Affordability index.\n",
    "\n",
    "Our strategy will be to collect the data from [Nomis](https://www.nomisweb.co.uk/query/construct/apilinks.asp?menuopt=201) for LEPS.\n",
    "\n",
    "Unfortunately the data is not available at the NUTS2 so we will have to use an alternative source\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../notebook_preamble.ipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "from zipfile import ZipFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dirs(name,dirs = ['raw','processed']):\n",
    "    '''\n",
    "    Utility that creates directories to save the data\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    for d in dirs:\n",
    "        if name not in os.listdir(f'../../data/{d}'):\n",
    "            os.mkdir(f'../../data/{d}/{name}')\n",
    "            \n",
    "def flat_freq(a_list):\n",
    "    '''\n",
    "    Return value counts for categories in a nested list\n",
    "    \n",
    "    '''\n",
    "    return(pd.Series([x for el in a_list for x in el]).value_counts())\n",
    "\n",
    "        \n",
    "\n",
    "def flatten_list(a_list):\n",
    "    \n",
    "    return([x for el in a_list for x in el])\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data(df,name,path,today=today_str):\n",
    "    '''\n",
    "    Utility to save processed data quicker\n",
    "    \n",
    "    Arguments:\n",
    "        df (df) is the dataframe we want to save\n",
    "        name (str) is the name of the file\n",
    "        path (str) is the path where we want to save the file\n",
    "        today (str) is the day when the data is saved\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    df.to_csv(f'{path}/{today_str}_{name}.csv')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_process_ashe_place(api_link,var_name):\n",
    "    '''\n",
    "    This function collects and processes ashe place data\n",
    "    \n",
    "    Arguments:\n",
    "        api_link (str) is the endpoint we get the data from\n",
    "        var_name (str) is the name for the observed value variable\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    #Get the data\n",
    "    nomis_table = pd.read_csv(api_link)\n",
    "    \n",
    "    #tidy variable names\n",
    "    nomis_table.columns = [x.lower() for x in nomis_table.columns]\n",
    "    \n",
    "    #Some subseting of rows (ie we only keep the values)\n",
    "    nomis_values = nomis_table.loc[nomis_table['measures_name']=='Value']\n",
    "    \n",
    "    #Some subsetting of columns\n",
    "    nomis_filtered = nomis_values[['date_name','geography_name','geography_code','obs_value']]\n",
    "    \n",
    "    #Observed value\n",
    "    nomis_filtered.rename(columns={'obs_value':'var_name'})\n",
    "    \n",
    "    return(nomis_filtered)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_ashe_dump_data(path,file,occupation_list):\n",
    "    '''\n",
    "    This function collects and parses data from an ASHE occupation salary dump\n",
    "    \n",
    "    Arguments:\n",
    "        path (str) is the path where we have stored the excel files\n",
    "        file (str) is the name of the file\n",
    "        occupation_list (list) is the list of occupations that we will focus on    \n",
    "    \n",
    "    '''\n",
    "    #Extract the year from the file name\n",
    "    year = file.split(' ')[-1][:-4]\n",
    "    \n",
    "    print(year)\n",
    "    \n",
    "    \n",
    "    #Read the file. We are focusin on Full-Time to keep the indicator comparable with the LEPS. \n",
    "    #We are also subsetting to remove some information at the top / bottom / sides\n",
    "    \n",
    "    table = pd.read_excel(path+'/'+file,\n",
    "                    sheet_name='Full-Time',skiprows=4,na_values='x').iloc[:-5,:4]\n",
    "    \n",
    "    #Extract NUTS and occupations from the 'Descriptionc' field\n",
    "    \n",
    "    #We will use the fact that occupations are all Uppercase\n",
    "    \n",
    "    place_names = []\n",
    "    occ_names = []\n",
    "    \n",
    "    #We go through every description and if a word is all uppercase we put it in an occupation container,\n",
    "    #otherwise in a place container\n",
    "    \n",
    "    for category in table['Descriptionc']:\n",
    "        \n",
    "        split = category.split(' ')\n",
    "        \n",
    "        place =[]\n",
    "        occ = []\n",
    "        \n",
    "        for word in split:\n",
    "            if word.isupper()==False:\n",
    "                place.append(word)\n",
    "                \n",
    "            else:\n",
    "                occ.append(word)\n",
    "                \n",
    "        place_names.append(' '.join(place))\n",
    "        occ_names.append(' '.join(occ))\n",
    "        \n",
    "    #Assign the words we identified as places to NUTS2 removing a trailing comma\n",
    "    table['nuts_2'] = [x[:-1] for x in place_names]\n",
    "    \n",
    "    #Assign occupations\n",
    "    table['occupation'] = occ_names\n",
    "    \n",
    "    #Assign years\n",
    "    table['year']=year\n",
    "    \n",
    "    #Focus on occupations of interest\n",
    "    table_filter = table.loc[[x in occupation_list for x in table['occupation']]]\n",
    "    \n",
    "    #Clean the occupation name\n",
    "    table_filter['occupation'] =[x.lower() for x in table_filter['occupation']]\n",
    "    \n",
    "    #Rename the median variable\n",
    "    table_filter.rename(columns={'Median':'gross_annual_salary_median'},inplace=True)\n",
    "    \n",
    "    return(table_filter[['year','nuts_2','occupation','gross_annual_salary_median']])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dirs\n",
    "\n",
    "if 'ashe_place' not in os.listdir('../../data/raw'):\n",
    "    os.makedirs('../../data/raw/ashe_place')\n",
    "\n",
    "if 'ashe_place' not in os.listdir('../../data/processed/'):\n",
    "    os.makedirs('../../data/processed/ashe_place')\n",
    "\n",
    "#Path to save data:\n",
    "\n",
    "proc_path ='../../data/processed/ashe_place'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Collect data\n",
    "\n",
    "We collect the data from NOMIS.\n",
    "\n",
    "Note that we are collecting **annual gross salary** for full-time workers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LEPS\n",
    "\n",
    "The LEP case will be easy as the information is already available at the lep level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_lep_link = 'https://www.nomisweb.co.uk/api/v01/dataset/NM_30_1.data.csv?geography=1925185537,1925185575,1925185538...1925185543,1925185572,1925185544,1925185570,1925185545,1925185577,1925185553,1925185547...1925185549,1925185571,1925185569,1925185551,1925185552,1925185554,1925185558,1925185555...1925185557,1925185559,1925185560,1925185550,1925185576,1925185562,1925185573,1925185563...1925185568&date=latestMINUS4-latest&sex=8&item=2&pay=7&measures=20100,20701'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ashe_lep = get_process_ashe_place(api_lep_link,'gross_annual_salary_median')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ashe_lep.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NUTS2\n",
    "\n",
    "ASHE data are not available at the NUTS2 level and it is not trivial to convert LAD data into NUTS as we have done in other places (eg House Affordability) because the information is only available as median salaries. We could have used number of jobs & average salaries to calculate wage bills and recalculate salaries at the NUTS2 level but this would mean reporting averages rather than medians. \n",
    "\n",
    "For all these reasons, we end using a ASHE data dump at the ONS level available [here](https://www.ons.gov.uk/employmentandlabourmarket/peopleinwork/earningsandworkinghours/adhocs/009571annualsurveyofhoursandearningsasheestimatesofannualandhourlyearningsforindustryandoccupationbynuts2andnuts3uk2011to2017)\n",
    "\n",
    "Note that there are some concerns about the reliability of these indicators given small sample sizes etc. so any indicators built using this data should be treated with caution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download and extract the ASHE data\n",
    "data_link = 'https://www.ons.gov.uk/file?uri=/employmentandlabourmarket/peopleinwork/earningsandworkinghours/adhocs/009571annualsurveyofhoursandearningsasheestimatesofannualandhourlyearningsforindustryandoccupationbynuts2andnuts3uk2011to2017/k42forpublishing.zip'\n",
    "\n",
    "ashe_req = requests.get(data_link)\n",
    "ashe_zip = ZipFile(BytesIO(ashe_req.content))\n",
    "ashe_zip.extractall(path='../../data/raw/ashe_place/download')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The extracted data are a bunch of excel files with median data by occupation and industry between 2011 and 2017.\n",
    "\n",
    "We will focus on Science, Engineering and technology occupations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dir = os.listdir('../../data/raw/ashe_place/download/K42a - NUTS2 by occupation')\n",
    "\n",
    "#Files we want to consider\n",
    "my_files = [x for x in my_dir if ('Annual pay' in x) & (' CV' not in x)]\n",
    "\n",
    "my_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../../data/raw/ashe_place/download/K42a - NUTS2 by occupation'\n",
    "occ_list = ['SCIENCE, RESEARCH, ENGINEERING AND TECHNOLOGY PROFESSIONALS']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sci_median_salaries = pd.concat([parse_ashe_dump_data(path,file,occ_list) for file in my_files]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sci_median_salaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We fix a typo in one of the geographies (they switched the order of Bristol and Bath)\n",
    "sci_median_salaries['nuts_2'] = ['Gloucestershire, Wiltshire and Bath/Bristol area' \n",
    "                                 if x=='Gloucestershire, Wiltshire and Bristol/Bath area' else x for x in sci_median_salaries['nuts_2']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final processing\n",
    "\n",
    "Add NUTS2 codes to the table\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nuts_codes_url = 'https://opendata.arcgis.com/datasets/ded3b436114440e5a1561c1e53400803_0.geojson'\n",
    "\n",
    "nuts_codes_names = requests.get(nuts_codes_url).json()['features']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add NUTS2 codes\n",
    "nuts_names_to_codes = {x['properties']['NUTS218NM']:x['properties']['NUTS218CD'] for x in nuts_codes_names}\n",
    "\n",
    "#Label the table with 2018 NUTS codes. \n",
    "sci_median_salaries['nuts_2_codes'] = [nuts_names_to_codes[x] if x in nuts_names_to_codes.keys() else np.nan for x in sci_median_salaries['nuts_2']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(sci_median_salaries.loc[sci_median_salaries['nuts_2_codes'].isna()]['nuts_2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a small number of mismatched areas due to changes in NUTS, plus aggregate non-NUTS london codes. We need to decide what to do about these.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_data(ashe_lep,'ashe_lep_all_occupations',proc_path)\n",
    "\n",
    "save_data(sci_median_salaries,'ashe_nuts_2_sci_tech',proc_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
