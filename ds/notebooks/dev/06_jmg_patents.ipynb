{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Patent analysis\n",
    "\n",
    "We collect, integrate and enrich patent data for the BEIS dashboard.\n",
    "\n",
    "Patent data is complex. [Data dictionary](https://github.com/nestauk/patent_analysis/raw/master/references/patstat_data_dict.pdf) and [guide](https://github.com/nestauk/patent_analysis/raw/master/references/The_Patents_Guide_2nd_edition.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../notebook_preamble.ipy\n",
    "\n",
    "from data_getters.labs.core import download_file\n",
    "import random\n",
    "from ast import literal_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dirs(name,dirs = ['raw','processed']):\n",
    "    '''\n",
    "    Utility that creates directories to save the data\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    for d in dirs:\n",
    "        if name not in os.listdir(f'../../data/{d}'):\n",
    "            os.mkdir(f'../../data/{d}/{name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is to download the data from nesta data getters\n",
    "\n",
    "def patent_download(file_path=None, progress=True):\n",
    "    \"\"\" Fetch patent data\n",
    "\n",
    "    Repo: http://github.com/nestauk/patent_analysis\n",
    "    Commit: cb11b3f\n",
    "    File: https://github.com/nestauk/patent_analysis/blob/master/notebooks/02-jmg-patent_merge.ipynb\n",
    "\n",
    "    Args:\n",
    "        file_path (`str`, optional): Path to download to. If None, stream file.\n",
    "        progress (`bool`, optional): If `True` and `file_path` is not `None`,\n",
    "            display download progress.\n",
    "    \"\"\"\n",
    "    itemname = \"Scotland_temp/15_10_2019_patents_combined.csv\"\n",
    "    return download_file(itemname, file_path, progress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data_dict(table,name,path,sample=5):\n",
    "    '''\n",
    "    A function to output the form for a data dictionary\n",
    "    \n",
    "    Args:\n",
    "        -table (df) is the df we want to create the data dictionary for\n",
    "        -name (str) of the df\n",
    "        -path (str) is the place where we want to save the file\n",
    "        \n",
    "\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    types = [estimate_type(table[x],sample=sample) for x in table.columns]\n",
    "        \n",
    "    data_dict = pd.DataFrame()\n",
    "    data_dict['variable'] = table.columns\n",
    "        \n",
    "    data_dict['type'] = types\n",
    "    \n",
    "    data_dict['description'] = ['' for x in data_dict['variable']]\n",
    "        \n",
    "    out = os.path.join(path,f'{today_str}_{name}.csv')\n",
    "    \n",
    "    #print(data_dict.columns)\n",
    "    \n",
    "    data_dict.to_csv(out)\n",
    "    \n",
    "\n",
    "def estimate_type(variable,sample):\n",
    "    '''\n",
    "    Estimates the type of a column. \n",
    "\n",
    "    Args:\n",
    "        variable (iterable) with values\n",
    "        sample (n) is the number of values to test\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    selection = random.sample(list(variable),sample)\n",
    "    \n",
    "    types = pd.Series([type(x) for x in selection]).value_counts().sort_values(ascending=False)\n",
    "    \n",
    "    return(types.index[0])\n",
    "\n",
    "                \n",
    "def flat_freq(a_list):\n",
    "    '''\n",
    "    Return value counts for categories in a nested list\n",
    "    \n",
    "    '''\n",
    "    return(pd.Series([x for el in a_list for x in el]).value_counts())\n",
    "\n",
    "        \n",
    "\n",
    "def flatten_list(a_list):\n",
    "    \n",
    "    return([x for el in a_list for x in el])\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_patenting_in_nuts(df,variable,nuts_lookup,pat_fam_id='docdb_family_id'):\n",
    "    '''\n",
    "    This function creates counts of inventors and applicants in NUTS areas.\n",
    "    \n",
    "    Note that the NUTS areas are not available at an standardised level of resolution. We will prune the length of NUTS3 (length of code>4) and match with the \n",
    "    nuts2 code lookup. This also means we will throw away any patents that don't have better level of resolution than NUTS2.\n",
    "    \n",
    "    Args:\n",
    "        df (dataframe) is the df with the patent information. Each row is a patent id and the columns contain metadata, authorship etc.\n",
    "        variable (str) is the variable we want to use in the analysis\n",
    "        pat_fam_id (str) is the patent family variable that we want to focus on\n",
    "        nuts_lookup (dict) is the nuts2 code to name lookup\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    #Group by patent families\n",
    "    \n",
    "    #This gives us a set of nuts regions involved in a single invention. \n",
    "    #Note that this is binary (whether a nuts region participates in an invention, rather than the number of participants)\n",
    "    #That would require a different approach using a person - patent lookup\n",
    "    \n",
    "    \n",
    "    #All this drama is because we are concatenating lists, so we need to flatten them first\n",
    "    # This gives us the NUTS involved in a patent family\n",
    "    fam = df.dropna(\n",
    "        axis=0,subset=[variable]).groupby(pat_fam_id)[variable].apply(lambda x: list(set(flatten_list(list(x))))\n",
    "                                                                     ).reset_index(drop=False)\n",
    "    \n",
    "    \n",
    "    #We need the earliest application year for every patent id\n",
    "    fam_year_lookup = df.drop_duplicates(pat_fam_id).set_index(pat_fam_id)['earliest_publn_year'].to_dict()\n",
    "    \n",
    "    #This adds a year field to the fam df\n",
    "    fam['earliest_publn_year'] = [fam_year_lookup[x] for x in fam[pat_fam_id]]\n",
    "    \n",
    "    #In Fam, we have in some cases multiple NUTS per application. We extract them using the unfold field variable\n",
    "    #We also concatenate them\n",
    "    fam_unfolded = pd.concat([unfold_field(x,variable,'earliest_publn_year') for item,x in fam.iterrows()])\n",
    "    \n",
    "    #We extract the nuts 2 using nuts lookup. Note that there will be some missing values (eg the 'UK') value\n",
    "    fam_unfolded['nuts_2'] = [nuts_lookup[x] if x in nuts_lookup.keys() else np.nan for x in fam_unfolded[variable]]\n",
    "\n",
    "    #Crosstab to obtain the counts of NUTS2 per year\n",
    "    patent_year_counts = pd.crosstab(fam_unfolded['nuts_2'],fam_unfolded['earliest_publn_year']).reset_index(drop=False)\n",
    "\n",
    "    #And melt\n",
    "    patent_year_counts_long = patent_year_counts.melt(id_vars='nuts_2').set_index(['nuts_2','earliest_publn_year'])\n",
    "    \n",
    "    patent_year_counts_long.rename(columns={'value':variable+'_n'},inplace=True)\n",
    "    \n",
    "    return(patent_year_counts_long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unfold_field(pat_item,variable,year_var):\n",
    "    '''\n",
    "    Some of the family patents involve multiple NUTS. We need to extract those so that we have one NUTS to year.\n",
    "    \n",
    "    Args:\n",
    "        pat_item (df item) is an item with a list of nuts involved in a patent family and the earliest year when it was files\n",
    "        variable (str) is the name of the variable (could be inventor nuts) \n",
    "        year_var (str) is the name of the year variable\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    nuts = []\n",
    "    years = []\n",
    "    \n",
    "    for n in pat_item[variable]:\n",
    "        nuts.append(n)\n",
    "        years.append(pat_item[year_var])\n",
    "\n",
    "    out = pd.DataFrame({variable:nuts,year_var:years})\n",
    "    \n",
    "    return(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_indicator(table,target_path,var_lookup,year_var,nuts_var='nuts_code',nuts_spec=2018):\n",
    "    '''\n",
    "    We use this function to create and save indicators using our standardised format.\n",
    "    \n",
    "    Args:\n",
    "        table (df) is a df with relevant information\n",
    "        target_path (str) is the location of the directory where we want to save the data (includes interim and processed)\n",
    "        var_lookup (dict) is a lookup to rename the variable into our standardised name\n",
    "        year (str) is the name of the year variable\n",
    "        nuts_var (str) is the name of the NUTS code variable. We assume it is nuts_code\n",
    "        nuts_spec (y) is the value of the NUTS specification. We assume we are working with 2018 NUTS\n",
    "    \n",
    "    '''\n",
    "    #Copy\n",
    "    t = table.reset_index(drop=False)\n",
    "    \n",
    "    #Reset index (we assume that the index is the nuts code, var name and year - this might need to be changed)\n",
    "    \n",
    "    \n",
    "    #Process the interim data into an indicator\n",
    "    \n",
    "    #This is the variable name and code\n",
    "    var_name = list(var_lookup.keys())[0]\n",
    "    \n",
    "    var_code = list(var_lookup.values())[0]\n",
    "    \n",
    "    #Focus on those\n",
    "    t = t[[year_var,nuts_var,var_name]]\n",
    "    \n",
    "    #Add the nuts specification\n",
    "    t['nuts_year_spec'] = nuts_spec\n",
    "    \n",
    "    #Rename variables\n",
    "    t.rename(columns={var_name:var_code,year_var:'year',nuts_var:'nuts_id'},inplace=True)\n",
    "\n",
    "    \n",
    "    #Reorder variables\n",
    "    t = t[['year','nuts_id','nuts_year_spec',var_code]]\n",
    "    \n",
    "    print(t.head())\n",
    "    \n",
    "    #Save in the processed folder\n",
    "    t.to_csv(f'../../data/processed/{target_path}/{var_code}.csv',index=False)\n",
    "    \n",
    "def autonuts_folder(path):\n",
    "    '''\n",
    "    Applies autonuts to all the files in a folder\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    csvs = [x for x in os.listdir(path) if '.csv' in x]\n",
    "    \n",
    "    for x in csvs:\n",
    "        \n",
    "        print(x)\n",
    "        \n",
    "        table = pd.read_csv(os.path.join(path,x))\n",
    "        \n",
    "        an = auto_nuts2_uk(table)\n",
    "        \n",
    "        an.to_csv(os.path.join(path,x),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_dirs('patents',['raw','processed','interim'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Read data\n",
    "\n",
    "We read a patent dataset based on the processing and analysis that we undertook [here](https://github.com/nestauk/patent_analysis)\n",
    "\n",
    "In the patent file that we read every row is a patent application and the columns contain information about it. In some cases, the columns contain lists of applicants, IPC codes and other things.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_d = patent_download()\n",
    "\n",
    "p = pd.read_csv(p_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We need to parse some of the lists in the data\n",
    "\n",
    "#These are the list variables that we need to parse\n",
    "list_vars = ['appl_psn_name','appl_person_address','appl_laua','appl_lad_name','appl_uk_postcode_long',\n",
    "           'inv_psn_name','inv_person_address','inv_laua','inv_lad_name','inv_uk_postcode_long','tf_weight','tf_techn_field_nr', 'tf_techn_field', 'ipc_class_symbol_proc_10',\n",
    "            'appl_nuts_name','inv_nuts_name','appl_nuts','inv_nuts']\n",
    "\n",
    "\n",
    "for v in list_vars:\n",
    "    \n",
    "    print(v)\n",
    "    \n",
    "    \n",
    "    p[v] = [literal_eval(x) if pd.isnull(x)==False else np.nan for x in p[v]]\n",
    "    \n",
    "    #Bring back the misssing variables\n",
    "    if any(l in v for l in ['lad','laua','nuts','ttwa']):\n",
    "        p[v] = [np.nan if type(var)!=list else np.nan if all(x=='missing' for x in var) else var for var in p[v]]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p['appln_auth'].value_counts().head(n=10).plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p['earliest_publn_year'].value_counts().loc[np.arange(2011,2019)].plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.groupby('docdb_family_id')['earliest_publn_year'].min().value_counts().loc[np.arange(2011,2019)].plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_freq(p['inv_nuts'].dropna()).head(n=10).plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata: NUTS lookup\n",
    "\n",
    "Read [NUTS lookup](http://geoportal1-ons.opendata.arcgis.com/datasets/9b4c94e915c844adb11e15a4b1e1294d_0.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nuts = pd.read_csv('https://opendata.arcgis.com/datasets/d266cbe2179a4766b4de7c6e73b4a285_0.csv')\n",
    "#nuts = pd.read_csv('http://geoportal1-ons.opendata.arcgis.com/datasets/9b4c94e915c844adb11e15a4b1e1294d_0.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is a NUTS 2 lookup FOR 2015\n",
    "nuts_2_code_name_lookup = nuts.drop_duplicates('NUTS215CD').set_index('NUTS215CD')['NUTS215NM'].to_dict()\n",
    "\n",
    "#nuts_2_code_name_lookup = nuts.drop_duplicates('NUTS218CD').set_index('NUTS218CD')['NUTS218NM'].to_dict()\n",
    "\n",
    "#We create a lookup between NUTS2 and NUTS3 codes\n",
    "nuts_3_to_2 = nuts.set_index('NUTS315CD')['NUTS215CD'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is a NUTS code - lookup name for all NUTS codes regardless of their level\n",
    "with open('../../data/aux/patstat_nuts_lookup.json','r') as infile:\n",
    "    nuts_patstat_lookup = json.load(infile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Process data\n",
    "\n",
    "Create count of applications and inventions per NUTS area and year focusing on the earliest publication year. We focus on patent families to avoid double counting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat_nuts = pd.concat(\n",
    "    [count_patenting_in_nuts(p.loc[(p['earliest_publn_year']>2012)&(p['earliest_publn_year']<2019)],var,nuts_3_to_2) \n",
    "     for var in ['inv_nuts','appl_nuts']],axis=1).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat_nuts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting -many more applications than inventors in London"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Output indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from beis_indicators.utils.nuts_utils import auto_nuts2_uk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_indicator(pat_nuts.sort_values(['nuts_2','earliest_publn_year']),'patents',{'inv_nuts_n':'total_inventions'},'earliest_publn_year',nuts_var='nuts_2',nuts_spec=2013)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autonuts_folder('../../data/processed/patents/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = pd.read_csv('../../data/processed/patents/total_inventions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.iloc[:,1:].to_csv('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
